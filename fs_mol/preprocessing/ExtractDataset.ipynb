{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4940e9a",
   "metadata": {},
   "source": [
    "# Assemble a Few-Shot Learning Dataset of Molecules from ChEMBL\n",
    "\n",
    "Here we describe the procedure used to extract the final dataset. The final dataset was obtained through implementation of four key steps: \n",
    "\n",
    "1. Query ChEMBL to obtain initial raw data\n",
    "2. Clean the data to ensure good quality, and threshold to derive binary classification labels\n",
    "3. Selection of assays for use in the pretraining, vs. those selected as few-shot testing tasks and for validation.\n",
    "4. Featurization of the data to prepare suitable input to a range of models\n",
    "\n",
    "## 1. Querying ChEMBL\n",
    "\n",
    "Our initial query of ChEMBL selected only those assays that contain more than 32 datapoints. We accessed CHEMBL27 and seelcted all assays with more than 32 measurements. We record the assay ids and confidence scores, where confidence reflects the level of information about the target protein in the assay: '9' is a known single protein target, '0' is completely unknown, for instance it could be as broad as an entire tissue. \n",
    "\n",
    "The resulting list of assays (or indeed list of ChEMBL assay ids supplied following an alternative query fitting the user's needs) can be passed to the script `query.py`. \n",
    "\n",
    "Here we extract a range of fields detailed in `preprocessing/utils/queries.py`. We take a multiple option approach, as we recognise that not all entries in ChEMBL have complete protein target information. When no protein target information is available, the query is carried out for any other information that may be suitable for characterizing the assay such as the target cell type or tissue. \n",
    "\n",
    "As a result of this initial query, we obtained 36,093 separate raw assay files as csvs. The cleaning process we followed considerably reduces this count. \n",
    "\n",
    "## 2. Cleaning\n",
    "\n",
    "The cleaning procedure takes place in three keys stages, detailed in `preprocessing/clean.py`:\n",
    "\n",
    "1. Assays are selected to proceed to the next stage only if they reflect activity or inhibition measurements with units of \"%\", \"uM\" or \"nM\".\n",
    "2. SMILES are standardized, and XC50 (IC50 ir EC50) measurements are converted to -log10([C]/NM) prior to thresholding.\n",
    "3. A final (optional) thresholding step is applied. \n",
    "\n",
    "The standardization procedure for SMILES is as follows: \n",
    "\n",
    "- Remove salts\n",
    "- Disconnect any metallo-organic complexes\n",
    "- Make certain the correct ion is present\n",
    "- Choose the largest fragment if the SMILES string represents disconnected components\n",
    "- Remove excess charges\n",
    "- Choose the canonical tautomer\n",
    "\n",
    "Following this procedure, molecules are rejected with a molecular weight > 900 Da, and exact SMILES-value duplicate pairs are dropped within an assay. \n",
    "\n",
    "**De-duplication** of SMILES then accepts a degree of variation in the measured value for the same SMILES -- if a SMILES value is repeated in a dataframe, we accept measurements where the standard value measured is within the same order of magnitude, to fairly capture measurement noise. We reject all measurements for that SMILES if that is not the case. While this may reject stereoisomers with profoundly different chemical behaviors, we wish to remove erroneous measurements of other molecules. \n",
    "\n",
    "###  Thresholding\n",
    "\n",
    "Our thresholding proceeds via a automated procedure that attempts to adapt flexibly to each assay to ensure that we do not discount a number of measurements due to overly rigid thresholding rules. \n",
    "\n",
    "We take the median value of an assay's activity measurements, and use this as a threshold provided it is in the range 5 $\\le$ median(pXC) $\\le$ 7 for enzymes, or 4 $\\le$ median(pXC) $\\le$ 6 for all other assays. If the median is outside this range, we select PKX = 5.0 as our fixed threshold. \n",
    "\n",
    "With this threshold we are able to apply a binary activity label.\n",
    "\n",
    "## Assay Selection for train-valid-test split\n",
    "\n",
    "Our assay selection proceeds via examining the final sizes of the assays and their associated protein information. We begin with a list of 27004 assays for which cleaning did not result in removal of all data. Not all assays have available protein information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c461e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e09eca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), \"target_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "703f52fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5047500 measurements from our first pass of cleaning (cleaning_failed == False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {df.cleaned_size.sum()} measurements from our first pass of cleaning (cleaning_failed == False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb64cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([df.loc[df['target_id'].notna()].astype({\"target_id\": int}).astype({\"target_id\": str}), \n",
    "          df.loc[df['target_id'].isna()]],\n",
    "          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31b1a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2584 unique known targets\n"
     ]
    }
   ],
   "source": [
    "# first select out assays that are very small\n",
    "df = df[df.cleaned_size>=32]\n",
    "print(f\"We have {len(df[df.target_id.notna()].target_id.unique())} unique known targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfcf8b",
   "metadata": {},
   "source": [
    "TODO: we need a brief description here of how the EC numbers were assigned (from Nadine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ea178",
   "metadata": {},
   "source": [
    "To select test tasks, we require that they only have well known target ids, and since we also wish to categorise by EC number, we will select those for which a good EC number can be obtained. \n",
    "\n",
    "We first extract everything that cannot be included as a few-shot test task, which involves the cases of:\n",
    "- having no good EC number (NaN or EC number considered unreliable). \n",
    "- no single target ID available (eg. non-single-protein measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of those that have a target id, select out the data that has reliable EC classes\n",
    "\n",
    "target_filtered = filtered[filtered.target_id.notna()]\n",
    "target_filtered[target_filtered.reliable_target_EC_super.notna()]\n",
    "good_ecs = notnatargets[notnatargets.reliable_target_EC_super == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edeaa413",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-66664acfb332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpossible_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreliable_target_EC_super\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/metamol-env-test/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1535\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "possible_test = df.iloc[df.target_id.notna() and df.reliable_target_EC_super.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a186865c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4         True\n",
       "         ...  \n",
       "26993    False\n",
       "26994    False\n",
       "26995    False\n",
       "26996    False\n",
       "27003    False\n",
       "Name: target_id, Length: 24083, dtype: bool"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target_id.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53faca3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metamolenv",
   "language": "python",
   "name": "metamolenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
