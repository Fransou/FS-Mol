{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0670b5e2",
   "metadata": {},
   "source": [
    "# Specify, train, and evaluate a PyTorch-based model using AbstractTorchFSMolModel\n",
    "\n",
    "Flexible implementations of few-shot models can be evaluated according to our benchmarking procedure using the AbstractTorchFSMolModel base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c61a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../\"))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../fs_mol\"))\n",
    "\n",
    "from models import AbstractTorchFSMolModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062dae5",
   "metadata": {},
   "source": [
    "# 1a. Specifying a new model\n",
    "\n",
    "As an example of a model implementation making use of this tooling and the new dataset we turn to `gnn_multitask.py`. This contains all of the required methods: `forward`, `get_model_state`, `is_param_task_specific`, `load_model_weights`, `build_from_model_file`. \n",
    "\n",
    "**forward**\n",
    "\n",
    "The `GNNMultitaskModel` consists of an initial per-node linear projection layer, a shared `GNN` with a configuration specified by a `GNNConfig`, a readout layer that converts node representations to full graph embeddings, and a tail MLP that acts as a 'head' with the number of outputs determined by the number of tasks the model is being trained/evaluated on simultaneously (one output unit per task in this binary classification problem).\n",
    "\n",
    "The `GNNMulitaskModel` uses the `FSMolMultitaskBatch`, an implementation of `FSMolBatch` with an additional `sample_to_task_id` attribute that permits graph predictions to be used if they correspond to the correct task.\n",
    "\n",
    "The output of this method should be predictions that can be used to calculate a differentiable loss when combined with graph labels from the batch.\n",
    "\n",
    "**model state**\n",
    "\n",
    "This returns a dictionary which is used to save the model. For `GNNMultitaskModel` an additional configuration dictionary is stored. \n",
    "\n",
    "**task specific parameters**\n",
    "\n",
    "For eg. multitask models some model parameters may be task specific, and therefore not used in later evaluation steps. This method returns true for all Tail_MLP layers.\n",
    "\n",
    "**load model weights**\n",
    "\n",
    "Implements a model loading function, and in the multitask model example, the state of the optimizers at the time of saving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc17ff",
   "metadata": {},
   "source": [
    "# 1b. Loading a new model and dataset\n",
    "\n",
    "As an example, we will load a sample model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gnn_multitask import (\n",
    "    GNNMultitaskConfig,\n",
    "    GNNMultitaskModel,\n",
    "    GNNConfig,\n",
    "    create_model,\n",
    ")\n",
    "\n",
    "from data.fsmol_dataset import FSMolDataset, DataFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a914b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a full FSMolDataset (see the notebooks/dataset.ipynb)\n",
    "fsmol_dataset = FSMolDataset.from_directory(\n",
    "        directory=os.path.join(os.getcwd(), \"../dataset/\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an output directory in which to save a model\n",
    "out_dir = os.path.join(os.getcwd(), \"test\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# set up the model configuration that completely specifies a GNNMultitaskModel\n",
    "model_config = GNNMultitaskConfig(\n",
    "        num_tasks=fsmol_dataset.get_num_fold_tasks(DataFold.TRAIN), # task for every training task\n",
    "        node_feature_dim=32, # fixed in our data preprocessing\n",
    "        gnn_config=GNNConfig(\n",
    "            type=\"PNA\",\n",
    "            hidden_dim=128,\n",
    "            num_edge_types=3,\n",
    "            num_heads=4, \n",
    "            per_head_dim=64,\n",
    "            intermediate_dim=1024, # intermediate representation used in BOOM layer\n",
    "            message_function_depth=1,\n",
    "            num_layers=10, # number of gnn layers\n",
    "        ),\n",
    "        num_outputs=1,\n",
    "        readout_type=\"combined\",\n",
    "        readout_use_only_last_timestep=False, # use all intermediate GNN activations in the final readout\n",
    "        num_tail_layers=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of a model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model(model_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664436ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\tNum parameters {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"\\tDevice: {device}\")\n",
    "# print(f\"\\tModel:\\n{model}\") # prints out full description of model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691260d",
   "metadata": {},
   "source": [
    "# 2. Training an AbstractTorchFSMolModel\n",
    "\n",
    "The training of the model is demonstrated in `train_loop` of `abstract_torch_fsmol_model.py`. The training loop accepts:\n",
    "\n",
    "- the model\n",
    "- the components associated with optimization, that is, the optimizer and learning rate scheduler if it exists\n",
    "- the training data as an iterable over minibatches\n",
    "- a validation function callable, that allows validation to be performed throughout metatraining. For example, by performing adaptation to meta-validation tasks.\n",
    "- a selection of other parameters including model saving directory, training patience, and logging of training values.\n",
    "\n",
    "To demonstrate the creation of a suitable training batch iterable, we take the example here of a `MultitaskTaskSampleBatchIterable`, which simply uses `FSMolDataset` to build an iterable over the `FSMolMultitaskBatch` we mention above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.multitask import MultitaskTaskSampleBatchIterable\n",
    "\n",
    "# note we need this dictionary to connect task numbers with names\n",
    "train_task_name_to_id = {\n",
    "        name: i for i, name in enumerate(fsmol_dataset.get_task_names(data_fold=DataFold.TRAIN))\n",
    "    }\n",
    "\n",
    "train_data=MultitaskTaskSampleBatchIterable(\n",
    "            fsmol_dataset,\n",
    "            data_fold=DataFold.TRAIN,\n",
    "            task_name_to_id=train_task_name_to_id,\n",
    "            max_num_graphs=256,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454500d",
   "metadata": {},
   "source": [
    "An iterable such as `train_data` is required input to `run_on_data_iterable`, which simply runs the model on the data loader. We suggest `train_loop` as a good example outline for using the `FS-Mol` dataloaders.\n",
    "\n",
    "**Validation function**\n",
    "\n",
    "Many validation modalities may be desired, however we give an example of validation-by-finetuning on a set of validation tasks.\n",
    "\n",
    "The validation method is provided in this instance as a callable to the training loop. \n",
    "\n",
    "We discuss in more detail below the validation function as it contains many similarities to our model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.multitask_train import validate_by_finetuning_on_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338fb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# create a validation callable that, when called on the model \n",
    "# using valid_fn(model), it returns a metric of performance on validation tasks (note that the FSMolDataset has been passed\n",
    "# already).\n",
    "valid_fn = partial(\n",
    "        validate_by_finetuning_on_tasks,\n",
    "        dataset=fsmol_dataset,\n",
    "        learning_rate=0.00005,\n",
    "        task_specific_learning_rate=0.0001,\n",
    "        batch_size=256,\n",
    "        metric_to_use=\"avg_precision\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49718965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.abstract_torch_fsmol_model import (\n",
    "    train_loop,\n",
    "    create_optimizer,\n",
    ")\n",
    "\n",
    "# create a specific optimizer with learning rate for training\n",
    "optimizer, lr_scheduler = create_optimizer(\n",
    "        model,\n",
    "        lr=0.00005,\n",
    "        task_specific_lr=0.0001, # we allow task specific layers to adapt faster than the core GNN here\n",
    "        warmup_steps=100,\n",
    "        task_specific_warmup_steps=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a training loop on the model -- it is saved as best_model.pt, any improvement on the validation metric\n",
    "# results in the saving of new weights.\n",
    "best_metric = train_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        train_data=train_data,\n",
    "        valid_fn=valid_fn,\n",
    "        output_folder=out_dir,\n",
    "        max_num_epochs=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75bfde7",
   "metadata": {},
   "source": [
    "The above example made use of a validation callable, `validate_by_finetuning_on_tasks` where a copy of the model is finetuned on a new validation task. This is performed over all validation tasks available from `fsmol_dataset` to return an aggregate metric representing the performance across all validation tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f67b00",
   "metadata": {},
   "source": [
    "# 3. Evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4e071",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The eval_model() function\n",
    "\n",
    "`fs_mol.utils.test_utils.eval_model()` is a general purpose model evaluation function that allows all models to be run against the full set of testing tasks. This method requires the following input:\n",
    "\n",
    "1. A test_model_fn callable: this helper function accepts a `FSMolTaskSample` (that is, a sample of datapoints from a single task, which will be created by a `TaskSampler`, this is a list of `MoleculeDatapoints` -- see notebooks/dataset.ipynb).\n",
    "    - The callable should accept a task sample and operate on it with the model to return a BinaryEvalMetric object containing all metrics calculated from the model output and labels of the task sample.\n",
    "    - It may also except other arguments which can be defined without being passed directly to eval_model. See `fs_mol/baseline_test.py` for a simple example.\n",
    "\n",
    "2. An `FSMolDataset` containing information about all tasks, from which a file reading iterable over tasks is made, upon which the TaskSampler acts to produce the necessary samples. If a single task only is being evaluated, for example if it is passed from the command line, the FSMolDataset will contain only that single task. \n",
    "\n",
    "3. A task_reader_fn callable: this simply is passed to `dataset.get_task_reading_iterable()` to perform additional transformations of the data as it is read from disk. The default case simple loads the data in to `FSMolTask` objects with no further changes. However, the user may define alternative readers.\n",
    "\n",
    "4. train_set_sample_sizes: also known as support set sample sizes. This is supplied as a list, in recognition that a model can be evaluated at multiple sizes of support set. This happens in both validation and final benchmarking evaluation. \n",
    "\n",
    "5. output directory: if this passed, a summary of the evaluation is written to a csv. Beware: running this during training will results in repeated overwrites. \n",
    "\n",
    "6. num_samples: the number of random splits to draw for the task undergoing evaluation. \n",
    "\n",
    "7. The data fold here is used to decide whether to shuffle the order of tasks evaluated, but this is only used in the case of the training loop.\n",
    "\n",
    "The returned results contain a list of evaluation metrics for each task, in a dictionary indexed by task name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ce6cb",
   "metadata": {},
   "source": [
    "The `validate_by_finetuning_on_tasks` example of `multitask_train.py` shows a further extension of the general `test_model_fn` -- provided the method returns a `BinaryEvalMetric` for the `FSMolTaskSample` passed to it by `eval_model` it will be possible to evaluate in the manner of `FS-Mol` benchmarking. \n",
    "\n",
    "In this case, the evaluation method rebuilds a fresh copy of the model from file and performs finetuning using the `FSMolTaskSample` passed by `eval_model`. The train loop carefully uses the batching machinery described elsewhere to ensure the model is passed correctly batched data, and rearranges the validation metrics in to the required `BinaryEvalMetrics`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0137a5",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73c595",
   "metadata": {},
   "source": [
    "A simple example is given in `baseline_test.py`. Here the test function builds a baseline kNN model, and evaluates the model on each `TaskSample`, returning a `BinaryEvalMetric`. The eval_model function collects the results by running over all test tasks, and repeated draws from the same task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.utils.metrics import BinaryEvalMetrics\n",
    "from fs_mol.baseline_test import test\n",
    "from fs_mol.data.fsmol_task import FSMolTask, FSMolTaskSample\n",
    "\n",
    "# testing function for a single task with the correct signature\n",
    "def test_model_fn(\n",
    "        task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    "    ) -> BinaryEvalMetrics:\n",
    "        return test(\n",
    "            model_name=\"kNN\",\n",
    "            task_sample=task_sample,\n",
    "            use_grid_search=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.utils.test_utils import eval_model\n",
    "\n",
    "eval_model(\n",
    "    test_model_fn=test_model_fn,\n",
    "    dataset=fsmol_dataset,\n",
    "    train_set_sample_sizes=[16],\n",
    "    out_dir=out_dir,\n",
    "    fold=DataFold.TEST,\n",
    "    num_samples=1,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77418dce",
   "metadata": {},
   "source": [
    "Evaluation of a model requires that it can use eval_model in the same manner, and therefore a test_model_fn must be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6d049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsmol",
   "language": "python",
   "name": "fsmol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
