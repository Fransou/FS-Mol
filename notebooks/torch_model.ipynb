{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0670b5e2",
   "metadata": {},
   "source": [
    "# Specify, train, and evaluate a PyTorch-based model using AbstractTorchFSMolModel\n",
    "\n",
    "Flexible implementations of few-shot models can be evaluated according to our benchmarking procedure using the AbstractTorchFSMolModel base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c61a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../\"))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../fs_mol\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062dae5",
   "metadata": {},
   "source": [
    "# 1a. Specifying a new model\n",
    "\n",
    "As an example of a model implementation making use of this tooling and the new dataset we turn to `gnn_multitask.py` as an example implementation of `AbstractTorchFSMolModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b07bf0",
   "metadata": {},
   "source": [
    "```python\n",
    "class AbstractTorchFSMolModel(Generic[BatchFeaturesType], torch.nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, batch: BatchFeaturesType) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_model_state(self) -> Dict[str, Any]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_param_task_specific(self, param_name: str) -> bool:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model_weights(\n",
    "        self,\n",
    "        path: str,\n",
    "        load_task_specific_weights: bool,\n",
    "        quiet: bool = False,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Load model weights from a saved checkpoint.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def build_from_model_file(\n",
    "        cls,\n",
    "        model_file: str,\n",
    "        config_overrides: Dict[str, Any] = {},\n",
    "        quiet: bool = False,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> AbstractTorchFSMolModel[BatchFeaturesType]:\n",
    "        \"\"\"Build the model architecture based on a saved checkpoint.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb207248",
   "metadata": {},
   "source": [
    "**forward**\n",
    "\n",
    "The `GNNMultitaskModel` consists of an initial per-node linear projection layer, a shared `GNN` with a configuration specified by a `GNNConfig`, a readout layer that converts node representations to full graph embeddings, and a tail MLP that acts as a 'head' with the number of outputs determined by the number of tasks the model is being trained/evaluated on simultaneously (one output unit per task in this binary classification problem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb337be",
   "metadata": {},
   "source": [
    "As discussed in the dataset notebook, the `GNNMulitaskModel` uses the `FSMolMultitaskBatch`, an implementation of `FSMolBatch` with an additional `sample_to_task_id` attribute that propagates task id through the model to allow selection on the outputs.\n",
    "\n",
    "The output of this method should be predictions that can be used to calculate a differentiable loss when combined with graph labels from the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376711aa",
   "metadata": {},
   "source": [
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class FSMolBatch:\n",
    "    num_graphs: int\n",
    "    num_nodes: int\n",
    "    num_edges: int\n",
    "    node_features: np.ndarray  # [V, atom_features] float\n",
    "    adjacency_lists: List[\n",
    "        np.ndarray\n",
    "    ]  # list, len num_edge_types, elements [num edges, 2] int tensors\n",
    "    edge_features: List[\n",
    "        np.ndarray\n",
    "    ]  # list, len num_edge_types, elements [num edges, ED] float tensors\n",
    "    node_to_graph: np.ndarray  # [V] long\n",
    "@dataclass(frozen=True)\n",
    "class FSMolMultitaskBatch(FSMolBatch):\n",
    "    sample_to_task_id: np.ndarray\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a49326",
   "metadata": {},
   "source": [
    "**model state**\n",
    "\n",
    "This returns a dictionary which is used to save the model. For `GNNMultitaskModel` an additional configuration dictionary is stored. \n",
    "\n",
    "**task specific parameters**\n",
    "\n",
    "For e.g. multitask models some model parameters may be task specific, and therefore not used in later evaluation steps. For `GNNMultitaskModel` this method returns true for all Tail_MLP layers.\n",
    "\n",
    "**load model weights**\n",
    "\n",
    "Implements a model loading function, and in the multitask model example, the state of the optimizers at the time of saving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc17ff",
   "metadata": {},
   "source": [
    "# 1b. Loading a new model and dataset\n",
    "\n",
    "As an example, we will load a sample model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d498db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gnn_multitask import (\n",
    "    GNNMultitaskConfig,\n",
    "    GNNMultitaskModel,\n",
    "    GNNConfig,\n",
    "    create_model,\n",
    ")\n",
    "\n",
    "from data.fsmol_dataset import FSMolDataset, DataFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927910ec",
   "metadata": {},
   "source": [
    "We use a `FSMolDataset` (see the notebooks/dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a914b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsmol_dataset = FSMolDataset.from_directory(\n",
    "        directory=os.path.join(os.getcwd(), \"../dataset/\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d7dd4",
   "metadata": {},
   "source": [
    "The `GNNMultitaskModel` uses a `GNNMultitaskConfig` to specify the model. Since it is based on the `GNN` module, this contains a `GNNConfig` which is accepted by the initialisation of a `GNN` module, a general purpose GNN module. \n",
    "\n",
    "```python\n",
    "\n",
    "@dataclass\n",
    "class GNNMultitaskConfig:\n",
    "    num_tasks: int\n",
    "    gnn_config: GNNConfig\n",
    "    node_feature_dim: int = 32\n",
    "    num_outputs: int = 1\n",
    "    readout_type: str = \"sum\"\n",
    "    readout_use_only_last_timestep: bool = False\n",
    "    readout_dim: Optional[int] = None\n",
    "    readout_num_heads: int = 12\n",
    "    readout_head_dim: int = 64\n",
    "    num_tail_layers: int = 1\n",
    "\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    type: str = \"MultiHeadAttention\"\n",
    "    num_edge_types: int = 3\n",
    "    hidden_dim: int = 128\n",
    "    num_heads: int = 4\n",
    "    per_head_dim: int = 32\n",
    "    intermediate_dim: int = 512\n",
    "    message_function_depth: int = 1\n",
    "    num_layers: int = 8\n",
    "    dropout_rate: float = 0.0\n",
    "    use_rezero_scaling: bool = True\n",
    "    make_edges_bidirectional: bool = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431cddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an output directory in which to save a model\n",
    "out_dir = os.path.join(os.getcwd(), \"test\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# set up the model configuration that completely specifies a GNNMultitaskModel\n",
    "model_config = GNNMultitaskConfig(\n",
    "        num_tasks=fsmol_dataset.get_num_fold_tasks(DataFold.TRAIN), # task for every training task\n",
    "        node_feature_dim=32, # fixed in our data preprocessing\n",
    "        gnn_config=GNNConfig(\n",
    "            type=\"PNA\",\n",
    "            hidden_dim=128,\n",
    "            num_edge_types=3,\n",
    "            num_heads=4, \n",
    "            per_head_dim=64,\n",
    "            intermediate_dim=1024, # intermediate representation used in BOOM layer\n",
    "            message_function_depth=1,\n",
    "            num_layers=10, # number of gnn layers\n",
    "        ),\n",
    "        num_outputs=1,\n",
    "        readout_type=\"combined\",\n",
    "        readout_use_only_last_timestep=False, # use all intermediate GNN activations in the final readout\n",
    "        num_tail_layers=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f515579",
   "metadata": {},
   "source": [
    "We build a model using the configuration specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2f91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of a model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model(model_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664436ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNum parameters 18690924\n",
      "\tDevice: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNum parameters {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"\\tDevice: {device}\")\n",
    "# print(f\"\\tModel:\\n{model}\") # prints out full description of model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691260d",
   "metadata": {},
   "source": [
    "# 2. Training an AbstractTorchFSMolModel\n",
    "\n",
    "A an example, the training of the model is demonstrated in `train_loop` of `abstract_torch_fsmol_model.py`. The training loop accepts:\n",
    "\n",
    "- the model\n",
    "- the components associated with optimization, that is, the optimizer and learning rate scheduler if it exists\n",
    "- the training data as an iterable over minibatches\n",
    "- a validation function callable, that allows validation to be performed throughout metatraining. For example, by performing adaptation to meta-validation tasks.\n",
    "- a selection of other parameters including model saving directory, training patience, and logging of training values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb2555",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_loop(\n",
    "    model: AbstractTorchFSMolModel[BatchFeaturesType],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    train_data: Iterable[Tuple[BatchFeaturesType, np.ndarray]],\n",
    "    valid_fn: Callable[[AbstractTorchFSMolModel[BatchFeaturesType]], float],\n",
    "    output_folder: str,\n",
    "    metric_to_use: MetricType = \"avg_precision\",\n",
    "    max_num_epochs: int = 100,\n",
    "    patience: int = 5,\n",
    "    aml_run=None,\n",
    "    quiet: bool = False,\n",
    "):\n",
    "    if quiet:\n",
    "        log_level = logging.DEBUG\n",
    "    else:\n",
    "        log_level = logging.INFO\n",
    "    initial_valid_metric = float(\"-inf\")\n",
    "    best_valid_metric = initial_valid_metric\n",
    "    logger.log(log_level, f\"  Initial validation metric: {best_valid_metric:.5f}\")\n",
    "\n",
    "    save_model(os.path.join(output_folder, \"best_model.pt\"), model, optimizer, -1)\n",
    "\n",
    "    epochs_since_best = 0\n",
    "    for epoch in range(0, max_num_epochs):\n",
    "        logger.log(log_level, f\"== Epoch {epoch}\")\n",
    "        logger.log(log_level, f\"  = Training\")\n",
    "        train_loss, train_metrics = run_on_data_iterable(\n",
    "            model,\n",
    "            data_iterable=train_data,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            quiet=quiet,\n",
    "            metric_name_prefix=\"train_\",\n",
    "            aml_run=aml_run,\n",
    "        )\n",
    "        mean_train_metric = np.mean(\n",
    "            [getattr(task_metrics, metric_to_use) for task_metrics in train_metrics.values()]\n",
    "        )\n",
    "        logger.log(log_level, f\"  Mean train loss: {train_loss:.5f}\")\n",
    "        logger.log(log_level, f\"  Mean train {metric_to_use}: {mean_train_metric:.5f}\")\n",
    "        logger.log(log_level, f\"  = Validation\")\n",
    "        valid_metric = valid_fn(model)\n",
    "        logger.log(log_level, f\"  Validation metric: {valid_metric:.5f}\")\n",
    "\n",
    "        if valid_metric > best_valid_metric:\n",
    "            logger.log(\n",
    "                log_level,\n",
    "                f\"   New best validation result {valid_metric:.5f} (increased from {best_valid_metric:.5f}).\",\n",
    "            )\n",
    "            best_valid_metric = valid_metric\n",
    "            epochs_since_best = 0\n",
    "\n",
    "            save_model(os.path.join(output_folder, \"best_model.pt\"), model, optimizer, epoch)\n",
    "        else:\n",
    "            epochs_since_best += 1\n",
    "            logger.log(log_level, f\"   Now had {epochs_since_best} epochs since best result.\")\n",
    "            if epochs_since_best >= patience:\n",
    "                break\n",
    "\n",
    "    return best_valid_metric\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9510d7",
   "metadata": {},
   "source": [
    "As an example ofthe creation of a suitable training batch iterable, we take the example here of a `MultitaskTaskSampleBatchIterable`, which simply uses `FSMolDataset` to build an iterable over the `FSMolMultitaskBatch` we mention above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399e403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.multitask import MultitaskTaskSampleBatchIterable\n",
    "\n",
    "# note we need this dictionary to connect task numbers with names\n",
    "train_task_name_to_id = {\n",
    "        name: i for i, name in enumerate(fsmol_dataset.get_task_names(data_fold=DataFold.TRAIN))\n",
    "    }\n",
    "\n",
    "train_data=MultitaskTaskSampleBatchIterable(\n",
    "            fsmol_dataset,\n",
    "            data_fold=DataFold.TRAIN,\n",
    "            task_name_to_id=train_task_name_to_id,\n",
    "            max_num_graphs=256,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454500d",
   "metadata": {},
   "source": [
    "An iterable such as `train_data` is required input to `run_on_data_iterable`, which simply runs the model on the data loader. We suggest `train_loop` as a good example outline for using the `FS-Mol` dataloaders.\n",
    "\n",
    "**Validation function**\n",
    "\n",
    "Many validation modalities may be desired, however we give an example of validation-by-finetuning on a set of validation tasks.\n",
    "\n",
    "The validation method is provided in this instance as a callable to the training loop. \n",
    "\n",
    "We discuss in more detail below the validation function as it contains many similarities to our model evaluation function `eval_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab38c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.multitask_train import validate_by_finetuning_on_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338fb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# create a validation callable that, when called on the model \n",
    "# using valid_fn(model), it returns a metric of performance on validation tasks (note that the FSMolDataset has been passed\n",
    "# already).\n",
    "valid_fn = partial(\n",
    "        validate_by_finetuning_on_tasks,\n",
    "        dataset=fsmol_dataset,\n",
    "        learning_rate=0.00005,\n",
    "        task_specific_learning_rate=0.0001,\n",
    "        batch_size=256,\n",
    "        metric_to_use=\"avg_precision\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49718965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.abstract_torch_fsmol_model import (\n",
    "    train_loop,\n",
    "    create_optimizer,\n",
    ")\n",
    "\n",
    "# create a specific optimizer with learning rate for training\n",
    "optimizer, lr_scheduler = create_optimizer(\n",
    "        model,\n",
    "        lr=0.00005,\n",
    "        task_specific_lr=0.0001, # we allow task specific layers to adapt faster than the core GNN here\n",
    "        warmup_steps=100,\n",
    "        task_specific_warmup_steps=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a training loop on the model -- it is saved as best_model.pt, any improvement on the validation metric\n",
    "# results in the saving of new weights.\n",
    "best_metric = train_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        train_data=train_data,\n",
    "        valid_fn=valid_fn,\n",
    "        output_folder=out_dir,\n",
    "        max_num_epochs=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75bfde7",
   "metadata": {},
   "source": [
    "The above example made use of a validation callable, `validate_by_finetuning_on_tasks` where a copy of the model is finetuned on a new validation task. This is performed over all validation tasks available from `fsmol_dataset` to return an aggregate metric representing the performance across all validation tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f67b00",
   "metadata": {},
   "source": [
    "# 3. Evaluating a model -- Benchmarking Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4e071",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The eval_model() function\n",
    "\n",
    "`fs_mol.utils.test_utils.eval_model()` is a general purpose model evaluation function that allows all models to be run against the full set of testing tasks. This method's behaviour represents the standard benchmarking procedure for any few-shot model on the test tasks.\n",
    "\n",
    "This method requires the following input:\n",
    "\n",
    "1. A test_model_fn callable: this helper function accepts a `FSMolTaskSample`  and returns test metrics for that sample.\n",
    "    - The callable should accept a task sample and operate on it with the model to return a `BinaryEvalMetric` object containing all metrics calculated from the model output and labels of the task sample.\n",
    "    \n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class BinaryEvalMetrics:\n",
    "    size: int\n",
    "    acc: float\n",
    "    balanced_acc: float\n",
    "    f1: float\n",
    "    prec: float\n",
    "    recall: float\n",
    "    roc_auc: float\n",
    "    avg_precision: float\n",
    "    kappa: float\n",
    "```\n",
    "See `fs_mol/baseline_test.py` for a simple example.\n",
    "\n",
    "2. An `FSMolDataset` containing information about all tasks, from which a file reading iterable over tasks is made.\n",
    "\n",
    "3. A task_reader_fn callable: this simply is passed to `dataset.get_task_reading_iterable()` to perform additional transformations of the data as it is read from disk, see datasets.ipynb. \n",
    "\n",
    "4. train_set_sample_sizes: also known as support set sample sizes. This is supplied as a list, in recognition that a model can be evaluated at multiple sizes of support set. This happens in both validation and final benchmarking evaluation. \n",
    "\n",
    "5. output directory: if this passed, a summary of the evaluation is written to a csv. Beware: running this during training will results in repeated overwrites. \n",
    "\n",
    "6. num_samples: the number of random splits to draw for the task undergoing evaluation. \n",
    "\n",
    "7. The data fold here is used to decide whether to shuffle the order of tasks evaluated, but this is only used in the case of the training loop.\n",
    "\n",
    "The returned results contain a list of evaluation metrics for each task, in a dictionary indexed by task name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45566d9",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def eval_model(\n",
    "    test_model_fn: Callable[[FSMolTaskSample, str, int], BinaryEvalMetrics],\n",
    "    dataset: FSMolDataset,\n",
    "    train_set_sample_sizes: List[int],\n",
    "    out_dir: Optional[str] = None,\n",
    "    num_samples: int = 5,\n",
    "    valid_size_or_ratio: Union[int, float] = 0.0,\n",
    "    test_size_or_ratio: Optional[Union[int, float, Tuple[int, int]]] = None,\n",
    "    fold: DataFold = DataFold.TEST,\n",
    "    task_reader_fn: Optional[Callable[[List[RichPath], int], Iterable[FSMolTask]]] = None,\n",
    "    seed: int = 0,\n",
    ") -> Dict[str, List[FSMolTaskSampleEvalResults]]:\n",
    "    \"\"\"Evaluate a model on the FSMolDataset passed.\n",
    "\n",
    "    Args:\n",
    "        test_model_fn: A callable directly evaluating the model of interest on a single task\n",
    "            sample in the form of an FSMolTaskSample. The test_model_fn should act on the task\n",
    "            sample with the model, using a temporary output folder and seed. All other required\n",
    "            variables should be defined in the same context as the callable. The function should\n",
    "            return a BinaryEvalMetrics object from the task.\n",
    "        dataset: An FSMolDataset with paths to the data to be evaluated supplied.\n",
    "        train_set_samples_sizes: List[int], a list of the support set sizes at which to evaluate,\n",
    "            this is the train_samples size in a TaskSample.\n",
    "        out_dir: final output directory for evaluation results.\n",
    "        num_samples: number of repeated draws from the task's data on which to evaluate the model.\n",
    "        valid_size_or_ratio: size of validation set in a TaskSample.\n",
    "        test_size_or_ratio: size of the test set in a TaskSample.\n",
    "        fold: the fold of FSMolDataset on which to perform evaluation, typically will be the test fold.\n",
    "        task_reader_fn: Callable allowing additional transformations on the data prior to its batching\n",
    "            and passing through a model.\n",
    "        seed: an base external seed value. Repeated runs vary from this seed.\n",
    "    \"\"\"\n",
    "    task_reading_kwargs = {\"task_reader_fn\": task_reader_fn} if task_reader_fn is not None else {}\n",
    "    task_to_results: Dict[str, List[FSMolTaskSampleEvalResults]] = {}\n",
    "\n",
    "    for task in dataset.get_task_reading_iterable(fold, **task_reading_kwargs):\n",
    "        test_results: List[FSMolTaskSampleEvalResults] = []\n",
    "        for train_size in train_set_sample_sizes:\n",
    "            task_sampler = StratifiedTaskSampler(\n",
    "                train_size_or_ratio=train_size,\n",
    "                valid_size_or_ratio=valid_size_or_ratio,\n",
    "                test_size_or_ratio=test_size_or_ratio,\n",
    "                allow_smaller_test=True,\n",
    "            )\n",
    "\n",
    "            for run_idx in range(num_samples):\n",
    "                logger.info(f\"=== Evaluating on {task.name}, #train {train_size}, run {run_idx}\")\n",
    "                with prefix_log_msgs(\n",
    "                    f\" Test - Task {task.name} - Size {train_size:3d} - Run {run_idx}\"\n",
    "                ), tempfile.TemporaryDirectory() as temp_out_folder:\n",
    "                    local_seed = seed + run_idx\n",
    "\n",
    "                    try:\n",
    "                        task_sample = task_sampler.sample(task, seed=local_seed)\n",
    "                    except (\n",
    "                        DatasetTooSmallException,\n",
    "                        DatasetClassTooSmallException,\n",
    "                        FoldTooSmallException,\n",
    "                        ValueError,\n",
    "                    ) as e:\n",
    "                        logger.warning(\n",
    "                            f\"Failed to draw sample with {train_size} train points for {task.name}. Skipping.\"\n",
    "                        )\n",
    "                        logger.debug(\"Sampling error: \" + str(e))\n",
    "                        continue\n",
    "\n",
    "                    test_metrics = test_model_fn(task_sample, temp_out_folder, local_seed)\n",
    "\n",
    "                    test_results.append(\n",
    "                        FSMolTaskSampleEvalResults(\n",
    "                            task_name=task.name,\n",
    "                            seed=local_seed,\n",
    "                            num_train=train_size,\n",
    "                            num_test=len(task_sample.test_samples),\n",
    "                            fraction_pos_train=task_sample.train_pos_label_ratio,\n",
    "                            fraction_pos_test=task_sample.test_pos_label_ratio,\n",
    "                            **dataclasses.asdict(test_metrics),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        task_to_results[task.name] = test_results\n",
    "\n",
    "        if out_dir is not None:\n",
    "            write_csv_summary(os.path.join(out_dir, f\"{task.name}_eval_results.csv\"), test_results)\n",
    "\n",
    "    return task_to_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ce6cb",
   "metadata": {},
   "source": [
    "The `validate_by_finetuning_on_tasks` example of `multitask_train.py` shows a further extension of the general `test_model_fn` -- provided the method returns a `BinaryEvalMetric` for the `FSMolTaskSample` passed to it by `eval_model` it will be possible to evaluate in the manner of `FS-Mol` benchmarking. \n",
    "\n",
    "In this case, the evaluation method rebuilds a fresh copy of the model from file and performs finetuning using the `FSMolTaskSample` passed by `eval_model`. The train loop carefully uses the batching machinery described elsewhere to ensure the model is passed correctly batched data, and rearranges the validation metrics in to the required `BinaryEvalMetrics`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0137a5",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73c595",
   "metadata": {},
   "source": [
    "A simple example is given in `baseline_test.py`. Here the test function builds a baseline kNN model, and evaluates the model on each `TaskSample`, returning a `BinaryEvalMetric`. The eval_model function collects the results by running over all test tasks, and repeated draws from the same task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be5c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.utils.metrics import BinaryEvalMetrics\n",
    "from fs_mol.baseline_test import test\n",
    "from fs_mol.data.fsmol_task import FSMolTask, FSMolTaskSample\n",
    "\n",
    "# testing function for a single task with the correct signature\n",
    "def test_model_fn(\n",
    "        task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    "    ) -> BinaryEvalMetrics:\n",
    "        return test(\n",
    "            model_name=\"kNN\",\n",
    "            task_sample=task_sample,\n",
    "            use_grid_search=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e98e9",
   "metadata": {},
   "source": [
    "In the interests of time for evaluation, we define an `FSMolDataset` on a single example task: \n",
    "\n",
    "# TODO: do this and get this bit to work (using just the baselines test fn for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ee225b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_fn(\n",
    "        task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    "    ) -> BinaryEvalMetrics:\n",
    "        return eval_model_by_finetuning_on_task(\n",
    "            model_weights_file,\n",
    "            model_cls=GNNMultitaskModel,\n",
    "            task_sample=task_sample,\n",
    "            temp_out_folder=temp_out_folder,\n",
    "            batcher=get_multitask_inference_batcher(max_num_graphs=args.batch_size),\n",
    "            learning_rate=args.learning_rate,\n",
    "            task_specific_learning_rate=args.task_specific_lr,\n",
    "            metric_to_use=\"avg_precision\",\n",
    "            seed=seed,\n",
    "            quiet=True,\n",
    "            device=device,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f7c9a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 127391 threw <class 'FileNotFoundError'> exception when trying to read files [/home/megstanley/Projects/FS-Mol/../dataset/test/CHEMBL2219045.jsonl.gz].\n",
      "Process 127391 threw <class 'FileNotFoundError'> exception when trying to read files [/home/megstanley/Projects/FS-Mol/../dataset/test/CHEMBL4133035.jsonl.gz].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fs_mol.utils.test_utils import eval_model\n",
    "\n",
    "eval_model(\n",
    "    test_model_fn=test_model_fn,\n",
    "    dataset=data,\n",
    "    train_set_sample_sizes=[16],\n",
    "    out_dir=out_dir,\n",
    "    fold=DataFold.TEST,\n",
    "    num_samples=1,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77418dce",
   "metadata": {},
   "source": [
    "Evaluation of a model requires that it can use eval_model in the same manner, and therefore a test_model_fn must be defined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsmol",
   "language": "python",
   "name": "fsmol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
