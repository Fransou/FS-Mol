{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Few-Shot Model Evaluation Results\n",
    "\n",
    "Assembling plots from summary files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Setting up local details:\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"Projects\", \"FS-Mol\")\n",
    "FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'], \"Datasets\", \"FS-Mol\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)\n",
    "\n",
    "from fs_mol.plotting.utils import (\n",
    "    highlight_max_all, \n",
    "    plot_all_assays, \n",
    "    load_data,\n",
    "    expand_values,\n",
    "    plot_task_performances_by_id,\n",
    "    box_plot,\n",
    "    plot_by_size,\n",
    "    get_aggregates_across_sizes\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making summary files\n",
    "\n",
    "Summary files are obtained by running `fs_mol/plotting/collect_eval_runs.py` on the outputs of evaluation runs. If an evaluation output directory is \"evaluation_output_directory\" then summary files are created with: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! python fs_mol/plotting/collect_eval_runs.py {model_name} {evaluation_output_directory} --plot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The option `--plot` results in a plot across support set sizes for each few-shot testing task. Final summarized results will be found in \"evaluation_output_directory/summary/{model_name}_summary.csv\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the collated evaluation data\n",
    "\n",
    "Create a dictionary of all model summary .csvs to be compared. The csvs are the final summaries from `collect_eval_runs.py`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Configure this to contain all the models that you want to look at.\n",
    "# Dict keys are human readable names, values are the path to the summary produced by collect_eval_runs.py\n",
    "results_path = os.path.join(FS_MOL_DATASET_PATH, \"results\")\n",
    "\n",
    "# a dictionary summarising all models to be compared. Add new paths here as desired.\n",
    "model_summaries = {\n",
    "    \"GNN-MAML\": os.path.join(results_path, \"MAML-Support16_summary.csv\"),\n",
    "#     \"PN\": os.path.join(results_path, \"PN_summary.csv\"),\n",
    "    \"GNN-MT\": os.path.join(results_path, \"GNN-Multitask_summary.csv\"),\n",
    "    \"ST\": os.path.join(results_path, \"random_forest_summary.csv\"),\n",
    "    \"GNN-ST\": os.path.join(results_path, \"GNN-ST_summary.csv\"),\n",
    "    \"kNN\": os.path.join(results_path, \"kNN_summary.csv\"),\n",
    "#     \"MAT\": os.path.join(results_path, \"MAT_summary.csv\"),\n",
    "}\n",
    "# Generated plots will be stored here, if you want to keep them. None disables saving.\n",
    "plot_output_dir = os.path.join(results_path, \"plots\")\n",
    "os.makedirs(plot_output_dir, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = load_data(model_summaries)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Highlight the best result for each task"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "styled_df = data.style.apply(lambda row: highlight_max_all(row), axis=1)\n",
    "# To save for exporting purposes, uncomment this (requires `pip install xlsxwriter`)\n",
    "# styled_df.to_excel(os.path.join(plot_output_dir, f\"all_model_highlighted_comparison.xlsx\"), engine='xlsxwriter')\n",
    "\n",
    "styled_df"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# expand out from val +/- error format, and calculate delta AUPRC\n",
    "data = expand_values(data, model_summaries)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Overview over all Tasks\n",
    "\n",
    "This compares with the trivial baseline of using a weighted coinflip according to the class imbalance in the training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This uses Latex to render plots, and requires the type1ec and type1cm packages.\n",
    "# It also required the dvipng utility.\n",
    "# On Debian/Ubuntu-based systems, this can be installed using `apt install cm-super texlive-latex-extra dvipng`\n",
    "\n",
    "plot_task_performances_by_id(data, model_summaries, support_set_size = 16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Incorporate protein information\n",
    "\n",
    "Our test tasks have associated target protein information available. We can merge this data to allow plotting with specific EC number classes highlighted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "protein_path = os.path.join(FS_MOL_DATASET_PATH, \"targets\", \"test_proteins.csv\")\n",
    "ecs = pd.read_csv(protein_path)\n",
    "ecs[\"target_id\"] = ecs[\"target_id\"].astype(int).astype(str)\n",
    "ecs[\"chembl_id\"] = ecs[\"chembl_id\"].astype(str)\n",
    "ecs[\"TASK_ID\"] = ecs.apply(lambda row: row[\"chembl_id\"][6:], axis = 1)\n",
    "\n",
    "data = ecs.merge(data, on=\"TASK_ID\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the highlight class is the EC class that will be highlighted in the resulting plot. \n",
    "# You may wish to use this for comparison across different EC classes.\n",
    "plot_task_performances_by_id(data, model_summaries, support_set_size = 16, highlight_class = 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot for each task, comparing different models\n",
    "\n",
    "This makes an individual comparison plot over models for each few-shot testing task, across all support set sizes available"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_all_assays(data, model_summaries.keys(), results_dir = plot_output_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarise the overall performance in box plots\n",
    "\n",
    "This reproduces the model comparison box plots in the manuscript."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "box_plot(data, model_summaries, support_set_size = 16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aggregate as a function of the number of training points, across all categories\n",
    "\n",
    "Here the results are aggregated according to EC class, and across all classes. This is used to plot the variation of performance with support set size, comparing all models in the model_summaries dictionary. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aggregate_df = get_aggregates_across_sizes(data, model_summaries)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aggregate_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this function has the option to plot all classes separately.\n",
    "plot_by_size(aggregate_df, model_summaries, plot_output_dir = plot_output_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ranking\n",
    "\n",
    "Here we use [autorank](https://pypi.org/project/autorank/) for an appropriate comparison between all methods when evaluated on multiple tasks.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from autorank import autorank\n",
    "\n",
    "# select correct data to rank with autorank\n",
    "for size in [16]:\n",
    "\n",
    "    df = data[[x for x in list(data.columns) if x.startswith(f\"{size}\") and \"val\" in x and \"delta-auprc\" in x]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = autorank(df, verbose=False)\n",
    "result.rankdf[\"meanrank\"]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('fsmol-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "91c2cdb63b030871da94aa046e171a8c212268d3e9d71e3496f8c89eaedb0da0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}